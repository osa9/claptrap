"""LangGraph agent implementation for ClapTrap bot."""

import operator
import os
import traceback
from typing import Annotated, Any, Literal

from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel
from typing_extensions import TypedDict

from .memory import ConversationMemory
from .prompts import CLAPTRAP_SYSTEM_PROMPT, FUNCTION_CALLING_INSTRUCTIONS
from .tools.image_generation import create_image_generation_tool
from .tools.web_search import create_web_search_tool
from .tools.youtube_summary import create_youtube_summary_tool, extract_youtube_urls

# ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()


class ImageInfo(BaseModel):
    """ç”Ÿæˆã•ã‚ŒãŸç”»åƒã®æƒ…å ±"""

    prompt: str
    size: str
    quality: str
    b64_data: str  # base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒãƒ‡ãƒ¼ã‚¿


class AgentResponse(BaseModel):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã®å¿œç­”ãƒ‡ãƒ¼ã‚¿"""

    text: str
    has_image: bool = False
    image_info: ImageInfo | None = None
    channel_id: str
    user_id: str


class AgentState(TypedDict):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çŠ¶æ…‹ã‚’å®šç¾©"""

    messages: Annotated[list[BaseMessage], operator.add]
    channel_id: str
    user_id: str
    next_action: Literal["respond", "use_tools", "end"]
    tool_calls_made: int  # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å›æ•°ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
    generated_image: dict[str, str] | None  # ç”Ÿæˆã•ã‚ŒãŸç”»åƒãƒ‡ãƒ¼ã‚¿


class ClapTrapAgent:
    """ClapTrapãƒœãƒƒãƒˆã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    def __init__(self, memory_db_path: str = "./data/claptrap.db"):
        """
        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–

        Args:
            memory_db_path: ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹
        """
        self.memory = ConversationMemory(memory_db_path)
        # 2ã¤ã®LLMã‚’ä½œæˆ: ãƒãƒ£ãƒƒãƒˆç”¨ã¨ãƒ„ãƒ¼ãƒ«ç”¨
        self.llm_chat = self._create_llm_chat()
        self.llm_tools = self._create_llm_tools()
        self.tools = self._create_tools()
        self.graph = self._create_graph()

    def _create_venice_llm(self, model: str) -> ChatOpenAI:
        """Venice LLMã‚’ä½œæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
        venice_key = os.getenv("VENICE_API_KEY")
        if not venice_key:
            raise ValueError("VENICE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return ChatOpenAI(
            api_key=venice_key,
            base_url="https://api.venice.ai/api/v1",
            model=model,
            temperature=0.9,
            max_tokens=1000,
        )

    def _create_llm_chat(self) -> ChatAnthropic | ChatOpenAI:
        """ãƒãƒ£ãƒƒãƒˆç”¨LLMã‚’åˆæœŸåŒ–ï¼ˆã‚¢ãƒ³ã‚»ãƒ³ã‚µãƒ¼ãƒ‰å„ªå…ˆï¼‰"""
        if os.getenv("VENICE_API_KEY"):
            # Venice uncensored model (Function Callingéå¯¾å¿œ)
            return self._create_venice_llm(
                os.getenv("VENICE_MODEL_CHAT", "venice-uncensored")
            )
        elif model := os.getenv("CLAUDE_AGENT_MODEL"):
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if api_key:
                return ChatAnthropic(
                    api_key=api_key,
                    model=model,
                    temperature=0.9,
                    max_tokens=2000,
                )
            raise ValueError("ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        elif model := os.getenv("OPENAI_AGENT_MODEL"):
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                return ChatOpenAI(
                    api_key=api_key,
                    model=model,
                    temperature=0.9,
                    max_tokens=2000,
                )
        raise ValueError(
            "VENICE_API_KEY, CLAUDE_AGENT_MODEL, OPENAI_AGENT_MODEL ã®ã„ãšã‚Œã‹ãŒå¿…è¦ã§ã™"
        )

    def _create_llm_tools(self) -> ChatAnthropic | ChatOpenAI:
        """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ç”¨LLMã‚’åˆæœŸåŒ–ï¼ˆFunction Callingå¯¾å¿œï¼‰"""
        if os.getenv("VENICE_API_KEY"):
            # Venice with function calling support
            return self._create_venice_llm(
                os.getenv("VENICE_MODEL_TOOLS", "zai-org-glm-4.6")
            )
        # ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ãƒãƒ£ãƒƒãƒˆç”¨ã¨åŒã˜
        return self._create_llm_chat()

    def _create_tools(self) -> list[BaseTool]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆ"""
        return [
            create_web_search_tool(),
            create_image_generation_tool(),
            create_youtube_summary_tool(),
        ]

    def _create_graph(self) -> Any:
        """LangGraphã‚’æ§‹ç¯‰"""
        # ãƒ„ãƒ¼ãƒ«ç”¨LLMã«ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰ï¼ˆFunction Callingå¯¾å¿œãƒ¢ãƒ‡ãƒ«ï¼‰
        llm_with_tools = self.llm_tools.bind_tools(self.tools)

        # ãƒãƒ£ãƒƒãƒˆç”¨LLMï¼ˆã‚¢ãƒ³ã‚»ãƒ³ã‚µãƒ¼ãƒ‰ã€ãƒ„ãƒ¼ãƒ«ãªã—ï¼‰
        llm_chat = self.llm_chat

        # ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
        tool_node = ToolNode(self.tools)

        def tool_wrapper(state: AgentState) -> dict[str, Any]:
            """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼"""
            from .tools.image_generation import (
                clear_last_generated_image,
                get_last_generated_image,
            )

            result = tool_node.invoke(state)
            print(result)

            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å›æ•°ã‚’å¢—åŠ 
            result["tool_calls_made"] = state.get("tool_calls_made", 0) + 1

            # ç”»åƒç”Ÿæˆãƒ„ãƒ¼ãƒ«ãŒä½¿ã‚ã‚ŒãŸå ´åˆã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’çŠ¶æ…‹ã«ä¿å­˜
            for message in result.get("messages", []):
                if (
                    isinstance(message, ToolMessage)
                    and message.name == "generate_image"
                ):
                    image_data = get_last_generated_image()
                    if image_data:
                        result["generated_image"] = image_data
                        clear_last_generated_image()
                        print(f"ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’çŠ¶æ…‹ã«ä¿å­˜: {image_data['prompt']}")
                    break

            print(f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå®Œäº†ã€‚å›æ•°: {result['tool_calls_made']}")
            return result

        def should_continue(state: AgentState) -> Literal["use_tools", "end"]:
            """ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
            messages = state["messages"]
            last_message = messages[-1]
            tool_calls_made = state.get("tool_calls_made", 0)

            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            print(
                f"should_continue: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°={len(messages)}, "
                f"æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—={type(last_message).__name__}, "
                f"ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å›æ•°={tool_calls_made}"
            )

            # æ—¢ã«2å›ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦ã„ã‚‹å ´åˆã¯çµ‚äº†
            if tool_calls_made >= 3:
                print("ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ä¸Šé™ã«é”ã—ãŸãŸã‚çµ‚äº†")
                return "end"

            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹å ´åˆ
            if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                print(f"ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æ¤œå‡º: {len(last_message.tool_calls)}å€‹")
                return "use_tools"

            print("ä¼šè©±çµ‚äº†")
            return "end"

        def call_model(state: AgentState) -> dict[str, Any]:
            """LLMã‚’å‘¼ã³å‡ºã—"""
            channel_id = state["channel_id"]
            messages = state["messages"]
            tool_calls_made = state.get("tool_calls_made", 0)
            print("=======")
            print(messages)

            # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œæ¸ˆã¿ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            has_tool_results = any(isinstance(m, ToolMessage) for m in messages)

            # ãƒ¡ãƒ¢ãƒªã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            memory_context = self.memory.get_context_for_channel(channel_id)

            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            system_prompt = CLAPTRAP_SYSTEM_PROMPT
            if memory_context:
                system_prompt += f"\n\n=== ä¼šè©±å±¥æ­´ ===\n{memory_context}"

            # ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ãªå ´åˆã®ã¿ãƒ„ãƒ¼ãƒ«é–¢é€£ã®æŒ‡ç¤ºã‚’è¿½åŠ 
            if not has_tool_results:
                system_prompt += f"\n\n{FUNCTION_CALLING_INSTRUCTIONS}"

                # YouTube URLã®æ¤œå‡ºã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
                last_user_message = None
                for msg in reversed(messages):
                    if isinstance(msg, HumanMessage):
                        last_user_message = msg
                        break

                if last_user_message:
                    youtube_urls = extract_youtube_urls(last_user_message.content)
                    if youtube_urls:
                        system_prompt += (
                            f"\n\né‡è¦: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«YouTube URL "
                            f"({youtube_urls[0]}) ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚"
                            "summarize_youtube_videoãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦å‹•ç”»ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
                        )

            # ãƒ„ãƒ¼ãƒ«ä½¿ç”¨å¾Œã®å¿œç­”ã‚’ä¿è¨¼ã™ã‚‹æŒ‡ç¤ºã‚’è¿½åŠ 
            if has_tool_results:
                system_prompt += (
                    "\n\n**é‡è¦**: ãƒ„ãƒ¼ãƒ«ã®çµæœã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦"
                    "ClapTrapã®æ€§æ ¼ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚"
                    "çµ¶å¯¾ã«ç©ºã®å¿œç­”ã‚„ç„¡è¨€ã¯é¿ã‘ã¦ãã ã•ã„ã€‚"
                )

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰ï¼ˆLangChainãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã‚’ç¶­æŒï¼‰
            formatted_messages = []

            # ãƒ„ãƒ¼ãƒ«çµæœãŒã‚ã‚‹å ´åˆï¼ˆã‚¢ãƒ³ã‚»ãƒ³ã‚µãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰
            # -> ToolMessageã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œãªã„ã®ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦å¤‰æ›
            if has_tool_results:
                # æœ€åˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
                user_content = ""
                tool_results = []
                for msg in messages:
                    if isinstance(msg, HumanMessage):
                        user_content = msg.content
                    elif isinstance(msg, ToolMessage):
                        tool_results.append(msg.content)

                # ãƒ„ãƒ¼ãƒ«çµæœã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ æ¸ˆã¿ãªã®ã§ã€
                # ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ + ãƒ„ãƒ¼ãƒ«çµæœã‚’å«ã‚ã‚‹
                combined_content = user_content
                if tool_results:
                    combined_content += "\n\n=== ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœ ===\n"
                    combined_content += "\n".join(tool_results)

                formatted_messages = [HumanMessage(content=combined_content)]
            else:
                # é€šå¸¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ï¼ˆãƒ„ãƒ¼ãƒ«å¯¾å¿œãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰
                for msg in messages:
                    if isinstance(msg, HumanMessage):
                        formatted_messages.append(HumanMessage(content=msg.content))
                    elif isinstance(msg, AIMessage):
                        # AIMessageã®tool_callsã‚‚ä¿æŒ
                        if hasattr(msg, "tool_calls") and msg.tool_calls:
                            formatted_messages.append(
                                AIMessage(
                                    content=msg.content, tool_calls=msg.tool_calls
                                )
                            )
                        else:
                            formatted_messages.append(AIMessage(content=msg.content))
                    elif isinstance(msg, ToolMessage):
                        # ToolMessageã®tool_call_idã‚’ä¿æŒ
                        if hasattr(msg, "tool_call_id"):
                            formatted_messages.append(
                                ToolMessage(
                                    content=msg.content, tool_call_id=msg.tool_call_id
                                )
                            )

            # ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®å ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
            if not formatted_messages:
                formatted_messages.append(HumanMessage(content="ã‚„ã‚"))

            # ä½¿ç”¨ã™ã‚‹LLMã‚’é¸æŠ
            # - ãƒ„ãƒ¼ãƒ«çµæœãŒã‚ã‚‹å ´åˆ: llm_chat (ã‚¢ãƒ³ã‚»ãƒ³ã‚µãƒ¼ãƒ‰) ã§å¿œç­”ç”Ÿæˆ
            # - ãƒ„ãƒ¼ãƒ«çµæœãŒãªã„å ´åˆ: llm_with_tools (FCå¯¾å¿œ) ã§ãƒ„ãƒ¼ãƒ«åˆ¤å®š
            if has_tool_results:
                # ã‚¢ãƒ³ã‚»ãƒ³ã‚µãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã§å¿œç­”ç”Ÿæˆ
                print(
                    f"LLMé¸æŠ: ãƒãƒ£ãƒƒãƒˆç”¨ãƒ¢ãƒ‡ãƒ« (ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œæ¸ˆã¿: {tool_calls_made}å›)"
                )
                messages_with_system = [
                    SystemMessage(content=system_prompt)
                ] + formatted_messages
                response = llm_chat.invoke(messages_with_system)
            else:
                # ãƒ„ãƒ¼ãƒ«å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã§ãƒ„ãƒ¼ãƒ«åˆ¤å®š
                print("LLMé¸æŠ: ãƒ„ãƒ¼ãƒ«ç”¨ãƒ¢ãƒ‡ãƒ« (åˆå›å‘¼ã³å‡ºã—)")
                if isinstance(self.llm_tools, ChatOpenAI):
                    messages_with_system = [
                        SystemMessage(content=system_prompt)
                    ] + formatted_messages
                    response = llm_with_tools.invoke(messages_with_system)
                else:
                    # Anthropicã®å ´åˆã¯systemãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                    llm_with_system = llm_with_tools.bind(system=system_prompt)
                    response = llm_with_system.invoke(formatted_messages)

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ¤œè¨¼
            if not hasattr(response, "content") or not response.content:
                print(f"è­¦å‘Š: LLMãŒç©ºã®å¿œç­”ã‚’è¿”ã—ã¾ã—ãŸ: {response}")

            return {"messages": [response]}

        # ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
        workflow = StateGraph(AgentState)

        # ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("agent", call_model)
        workflow.add_node("tools", tool_wrapper)

        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’è¨­å®š
        workflow.set_entry_point("agent")

        # æ¡ä»¶åˆ†å²ã‚’è¿½åŠ 
        workflow.add_conditional_edges(
            "agent", should_continue, {"use_tools": "tools", "end": END}
        )

        # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æˆ»ã‚‹
        workflow.add_edge("tools", "agent")

        return workflow.compile()

    async def process_message(
        self, message: str, channel_id: str, user_id: str
    ) -> AgentResponse:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã¦å¿œç­”ã‚’ç”Ÿæˆ

        Args:
            message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            channel_id: Discordãƒãƒ£ãƒ³ãƒãƒ« ID
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ ID

        Returns:
            AgentResponse: æ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”ãƒ‡ãƒ¼ã‚¿
        """
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ¡ãƒ¢ãƒªã«è¿½åŠ 
        user_message = HumanMessage(content=message)
        self.memory.add_message(channel_id, user_message)

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
        initial_state: AgentState = {
            "messages": [user_message],
            "channel_id": channel_id,
            "user_id": user_id,
            "next_action": "respond",
            "tool_calls_made": 0,
            "generated_image": None,
        }

        try:
            # ã‚°ãƒ©ãƒ•ã‚’å®Ÿè¡Œï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ããŸã‚ã€é©åˆ‡ãªå®Ÿè¡Œå›æ•°ä¸Šé™ã‚’è¨­å®šï¼‰
            final_state = await self.graph.ainvoke(
                initial_state, config={"recursion_limit": 10}
            )

            # æœ€å¾Œã®AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
            ai_response = None

            for msg in reversed(final_state["messages"]):
                if isinstance(msg, AIMessage):
                    # ç©ºæ–‡å­—åˆ—ã‚‚æœ‰åŠ¹ãªå¿œç­”ã¨ã—ã¦æ‰±ã†
                    ai_response = msg.content if msg.content is not None else ""
                    break

            # ai_responseãŒNoneã®å ´åˆã®ã¿ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½¿ç”¨
            if ai_response is None:
                # ãƒ­ã‚°å‡ºåŠ›ã§å•é¡Œèª¿æŸ»ã‚’æ”¯æ´
                print(f"è­¦å‘Š: AIå¿œç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚final_state: {final_state}")
                ai_response = (
                    "ã†ãƒ¼ã‚“ã€ä½•ã ã‹èª¿å­ãŒæ‚ªã„ã¿ãŸã„ã ã‚ˆã…â€¦"
                    "ã‚‚ã†ä¸€åº¦è©±ã—ã‹ã‘ã¦ã¿ã¦ã»ã—ã„ã®ã ãƒ¼ï¼"
                )
            elif ai_response == "":
                # ç©ºæ–‡å­—åˆ—ã®å ´åˆã¯ã€ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ã‚’ç¤ºã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¤‰æ›
                print("è­¦å‘Š: AIå¿œç­”ãŒç©ºæ–‡å­—åˆ—ã§ã™ã€‚ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                ai_response = (
                    "ã‚ã‚Œã‚Œï¼Ÿä½•ã ã‹ãƒ„ãƒ¼ãƒ«ãŒã†ã¾ãå‹•ã‹ãªã‹ã£ãŸã¿ãŸã„ãªã®ã ã€œğŸ’¦\n"
                    "ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ã¿ã‚‹ã‹ã€åˆ¥ã®æ–¹æ³•ã§èã„ã¦ã¿ã¦ã»ã—ã„ã®ã ãƒ¼ï¼"
                )

            # å¿œç­”ã‚’ãƒ¡ãƒ¢ãƒªã«è¿½åŠ 
            ai_message = AIMessage(content=ai_response)
            self.memory.add_message(channel_id, ai_message)

            # çŠ¶æ…‹ã‹ã‚‰ç”»åƒæƒ…å ±ã‚’å–å¾—
            image_info = self._get_image_info_from_state(final_state)

            return AgentResponse(
                text=ai_response,
                has_image=image_info is not None,
                image_info=image_info,
                channel_id=channel_id,
                user_id=user_id,
            )

        except Exception as e:
            error_response = (
                "ã«ã‚ƒã«ã‚ƒã£ï¼ï¼Ÿã‚¯ãƒ©ãƒˆãƒ©ã¡ã‚ƒã‚“ã®å›è·¯ãŒã¡ã‚‡ã£ã¨ãƒã‚°ã£ãŸã®ã ã€œã€œğŸ’¦ "
            )
            print(f"ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            print(
                f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: '{message}', ãƒãƒ£ãƒ³ãƒãƒ«: {channel_id}, "
                f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_id}"
            )
            traceback.print_exc()

            # ã‚¨ãƒ©ãƒ¼å¿œç­”ã‚‚ãƒ¡ãƒ¢ãƒªã«è¿½åŠ 
            try:
                error_message = AIMessage(content=error_response)
                self.memory.add_message(channel_id, error_message)
            except Exception as memory_error:
                print(f"ãƒ¡ãƒ¢ãƒªè¿½åŠ ã‚¨ãƒ©ãƒ¼: {memory_error}")

            return AgentResponse(
                text=error_response,
                has_image=False,
                image_info=None,
                channel_id=channel_id,
                user_id=user_id,
            )

    def _get_image_info_from_state(self, state: AgentState) -> ImageInfo | None:
        """AgentStateã‹ã‚‰ç”»åƒæƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚"""
        image_data = state.get("generated_image")
        if image_data:
            return ImageInfo(
                prompt=image_data["prompt"],
                size=image_data["size"],
                quality=image_data["quality"],
                b64_data=image_data["b64_data"],
            )
        return None

    def clear_memory(self, channel_id: str) -> None:
        """æŒ‡å®šãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        self.memory.clear_channel_memory(channel_id)


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_agent_instance: ClapTrapAgent | None = None


def get_agent() -> ClapTrapAgent:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _agent_instance  # noqa: PLW0603
    if _agent_instance is None:
        _agent_instance = ClapTrapAgent()
    return _agent_instance


async def process_user_message(
    message: str, channel_id: str, user_id: str
) -> AgentResponse:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã™ã‚‹ä¾¿åˆ©é–¢æ•°

    Args:
        message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        channel_id: Discordãƒãƒ£ãƒ³ãƒãƒ« ID
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ ID

    Returns:
        AgentResponse: æ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”ãƒ‡ãƒ¼ã‚¿
    """
    agent = get_agent()
    return await agent.process_message(message, channel_id, user_id)
